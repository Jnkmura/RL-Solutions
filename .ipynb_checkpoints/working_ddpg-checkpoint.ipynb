{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Implementation of DDPG - Deep Deterministic Policy Gradient\n",
    "Algorithm and hyperparameter details can be found here: \n",
    "    http://arxiv.org/pdf/1509.02971v2.pdf\n",
    "The algorithm is tested on the Pendulum-v0 OpenAI gym task \n",
    "and developed with tflearn + Tensorflow\n",
    "Author: Patrick Emami\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym \n",
    "import tflearn\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "# ==========================\n",
    "#   Training Parameters\n",
    "# ==========================\n",
    "# Max training steps\n",
    "MAX_EPISODES = 50000\n",
    "# Max episode length\n",
    "MAX_EP_STEPS = 1000\n",
    "# Base learning rate for the Actor network\n",
    "ACTOR_LEARNING_RATE = 0.001\n",
    "# Base learning rate for the Critic Network\n",
    "CRITIC_LEARNING_RATE = 0.0001\n",
    "# Discount factor \n",
    "GAMMA = 0.99\n",
    "# Soft target update param\n",
    "TAU = 0.001\n",
    "\n",
    "# ===========================\n",
    "#   Utility Parameters\n",
    "# ===========================\n",
    "# Render gym env during training\n",
    "RENDER_ENV = True\n",
    "# Use Gym Monitor\n",
    "GYM_MONITOR_EN = True\n",
    "# Gym environment\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "# Directory for storing gym results\n",
    "MONITOR_DIR = './results/gym_ddpg'\n",
    "# Directory for storing tensorboard summary results\n",
    "SUMMARY_DIR = './results/tf_ddpg'\n",
    "RANDOM_SEED = 1234\n",
    "# Size of replay buffer\n",
    "BUFFER_SIZE = 10000\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "# ===========================\n",
    "#   Actor and Critic DNNs\n",
    "# ===========================\n",
    "class ActorNetwork(object):\n",
    "    \"\"\" \n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -2 and 2\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "        \n",
    "        self.target_network_params = tf.trainable_variables()[len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.mul(self.network_params[i], self.tau) + \\\n",
    "                tf.mul(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "        \n",
    "        # Combine the gradients here \n",
    "        self.actor_gradients = tf.gradients(self.scaled_out, self.network_params, -self.action_gradient)\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self): \n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400, activation='relu')\n",
    "        net = tflearn.fully_connected(net, 300, activation='relu')\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, self.a_dim, activation='tanh', weights_init=w_init)\n",
    "        scaled_out = tf.mul(out, self.action_bound) # Scale output to -action_bound to action_bound\n",
    "        return inputs, out, scaled_out \n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\" \n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "        \n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.mul(self.network_params[i], self.tau) + tf.mul(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "    \n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400, activation='relu')\n",
    "\n",
    "        # Add the action tensor in the 2nd hidden layer\n",
    "        # Use two temp layers to get the corresponding weights and biases\n",
    "        t1 = tflearn.fully_connected(net, 300)\n",
    "        t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "        net = tflearn.activation(tf.matmul(net,t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a) \n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions): \n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "# ===========================\n",
    "#   Tensorflow Summary Ops\n",
    "# ===========================\n",
    "def build_summaries(): \n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.scalar_summary(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.scalar_summary(\"Qmax Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.merge_all_summaries()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "# ===========================\n",
    "#   Agent Training\n",
    "# ===========================\n",
    "def train(sess, env, actor, critic):\n",
    "\n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter(SUMMARY_DIR, sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "\n",
    "    for i in xrange(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in xrange(MAX_EP_STEPS):\n",
    "\n",
    "            if RENDER_ENV: \n",
    "                env.render()\n",
    "\n",
    "            # Added exploration noise\n",
    "            a = actor.predict(np.reshape(s, (1, 3))) + (1. / (1. + i + j))\n",
    "\n",
    "            s2, r, terminal, info = env.step(a[0])\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r, \\\n",
    "                terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > MINIBATCH_SIZE:     \n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                    replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = critic.predict_target(s2_batch, actor.predict_target(s2_batch))\n",
    "\n",
    "                y_i = []\n",
    "                for k in xrange(MINIBATCH_SIZE):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + GAMMA * target_q[k])\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = critic.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)))\n",
    "            \n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                # Update the actor policy using the sampled gradient\n",
    "                a_outs = actor.predict(s_batch)                \n",
    "                grads = critic.action_gradients(s_batch, a_outs)\n",
    "                actor.train(s_batch, grads[0])\n",
    "\n",
    "                # Update target networks\n",
    "                actor.update_target_network()\n",
    "                critic.update_target_network()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal:\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j)\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, i)\n",
    "                writer.flush()\n",
    "\n",
    "                print '| Reward: %.2i' % int(ep_reward), \" | Episode\", i, \\\n",
    "                    '| Qmax: %.4f' % (ep_ave_max_q / float(j))\n",
    "\n",
    "                break\n",
    "def main(_):\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        env = gym.make(ENV_NAME)\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "        env.seed(RANDOM_SEED)\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        action_bound = env.action_space.high\n",
    "        # Ensure action bound is symmetric\n",
    "        assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "        actor = ActorNetwork(sess, state_dim, action_dim, action_bound, \\\n",
    "            ACTOR_LEARNING_RATE, TAU)\n",
    "\n",
    "        critic = CriticNetwork(sess, state_dim, action_dim, \\\n",
    "            CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())\n",
    "\n",
    "        if GYM_MONITOR_EN:\n",
    "            if not RENDER_ENV:\n",
    "                env.monitor.start(MONITOR_DIR, video_callable=False, force=True)\n",
    "            else:\n",
    "                env.monitor.start(MONITOR_DIR, force=True)\n",
    "\n",
    "        train(sess, env, actor, critic)\n",
    "\n",
    "        if GYM_MONITOR_EN:\n",
    "            env.monitor.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "rlsolutions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
