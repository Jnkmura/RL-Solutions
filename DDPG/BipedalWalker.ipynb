{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate, Add\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils.noise import OrnsteinUhlenbeckActionNoise\n",
    "from utils.experience_replay import ExpReplay\n",
    "from utils.models import Actor, Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \n",
    "    def __init__ (\n",
    "        self,\n",
    "        env,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        action_low,\n",
    "        action_high,\n",
    "        replaybuffer,\n",
    "        warm_steps = 50000,\n",
    "        tau = 0.001\n",
    "    ):\n",
    "        \n",
    "        self.env = env\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.warm_steps = warm_steps\n",
    "        self.replay = replaybuffer\n",
    "        self.tau = tau\n",
    "          \n",
    "        self.state_ph = tf.placeholder(tf.float32, ((None,) + state_dim))\n",
    "        self.action_ph = tf.placeholder(tf.float32, ((None,) + action_dim))\n",
    "        self.target_ph = tf.placeholder(tf.float32, (None, 1))  \n",
    "        self.action_grads_ph = tf.placeholder(tf.float32, ((None,) + action_dim)) \n",
    "        self.is_training_ph = tf.placeholder_with_default(True, shape=None)\n",
    "            \n",
    "        self.critic = Critic(self.state_ph, self.action_ph, state_dim, action_dim)\n",
    "        self.critic_target = Critic(self.state_ph, self.action_ph, state_dim, action_dim)    \n",
    "            \n",
    "        self.actor = Actor(self.state_ph, state_dim, action_dim, low, high)\n",
    "        self.actor_target = Actor(self.state_ph, state_dim, action_dim, low, high)\n",
    "        \n",
    "        self.critic_train_step = self.critic.train_step(self.target_ph)\n",
    "        self.actor_train_step = self.actor.train_step(self.action_grads_ph)\n",
    "        \n",
    "        self.update_critic_target = self.update_target_network(self.critic.network_params, self.critic_target.network_params, self.tau)\n",
    "        self.update_actor_target = self.update_target_network(self.actor.network_params, self.actor_target.network_params, self.tau)\n",
    "        \n",
    "    def update_target_network(self, network_params, target_network_params, tau):     \n",
    "        \n",
    "        op_holder = []\n",
    "        for from_var,to_var in zip(network_params, target_network_params):\n",
    "            op_holder.append(to_var.assign((tf.multiply(from_var, tau) + tf.multiply(to_var, 1. - tau))))        \n",
    "\n",
    "        return op_holder\n",
    "        \n",
    "    def train(self, env_name, train_eps = 5000, noise_scale = 0.1):\n",
    "        \n",
    "        start_ep = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        rewards = []\n",
    "        sess.run(tf.global_variables_initializer())   \n",
    "        writer = tf.summary.FileWriter(os.path.join('logs', env_name.lower(), start_time))\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        noise_scaling = noise_scale * (self.action_high - self.action_low)\n",
    "     \n",
    "        for random_step in range(1, self.warm_steps + 1):\n",
    "            action = self.env.action_space.sample()\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            self.replay.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "        \n",
    "        exploration_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(self.action_dim))\n",
    "\n",
    "        for train_ep in range(1, train_eps + 1):      \n",
    "            state = self.env.reset()\n",
    "            \n",
    "            exploration_noise.reset()\n",
    "            train_step = 0\n",
    "            episode_reward = 0\n",
    "            ep_done = False\n",
    "\n",
    "            while not ep_done:\n",
    "                train_step += 1           \n",
    "\n",
    "                action = sess.run(self.actor.output, {self.state_ph: state[None]})[0]     \n",
    "                \n",
    "                action += exploration_noise() * noise_scaling\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay.add(state, action, reward, next_state, done)\n",
    "\n",
    "                episode_reward += reward\n",
    "                states_batch, actions_batch, rewards_batch, next_states_batch, done_batch = self.replay.sample(64) \n",
    "\n",
    "                # Critic training step    \n",
    "                future_action = sess.run(self.actor_target.output, {self.state_ph: next_states_batch})  \n",
    "                future_Q = sess.run(self.critic_target.output, {self.state_ph: next_states_batch, self.action_ph: future_action})[:,0]   \n",
    "                future_Q[done_batch] = 0\n",
    "                targets = rewards_batch + (future_Q * 0.99)\n",
    "                sess.run(self.critic_train_step, {self.state_ph:states_batch, self.action_ph:actions_batch, self.target_ph:np.expand_dims(targets, 1)})   \n",
    "\n",
    "                # Actor training step\n",
    "                actor_actions = sess.run(self.actor.output, {self.state_ph:states_batch})\n",
    "                action_grads = sess.run(self.critic.action_grads, {self.state_ph:states_batch, self.action_ph:actor_actions})\n",
    "                sess.run(self.actor_train_step, {self.state_ph:states_batch, self.action_grads_ph:action_grads[0]})\n",
    "\n",
    "                # Update target networks\n",
    "                sess.run(self.update_critic_target)\n",
    "                sess.run(self.update_actor_target)\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "                if done or train_step == 1000:\n",
    "                    start_ep += 1\n",
    "                    summary=tf.Summary()\n",
    "                    summary.value.add(tag='Episode Rewards', simple_value = episode_reward)\n",
    "                    writer.add_summary(summary, start_ep)\n",
    "                    \n",
    "                    ep_done = True\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkj/.conda/envs/RLsolutions/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env_name = 'BipedalWalker-v2'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape\n",
    "high = env.action_space.high\n",
    "low = env.action_space.low\n",
    "\n",
    "replaybuffer = ExpReplay(5e6)\n",
    "ddpg = DDPG(env, state_dim, action_dim, high, low, replaybuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.train(env_name, train_eps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "rlsolutions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
