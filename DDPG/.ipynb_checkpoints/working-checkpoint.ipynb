{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay class that stores n size tuple of experiences\n",
    "class ExpReplay(object):\n",
    "    def __init__(self, size):\n",
    "        self._storage = []\n",
    "        self._maxsize = size       \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, s, a, r, next_s, done):\n",
    "        data = (s, a, r, next_s, done)\n",
    "        \n",
    "        self._storage.append(data)\n",
    "        storage_size = len(self._storage)\n",
    "        if (storage_size >= self._maxsize):\n",
    "            self._storage = self._storage[storage_size-self._maxsize:]\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.randint(0, len(self._storage), size=batch_size)\n",
    "        batch = np.array(self._storage)[idx]\n",
    "        states, actions, rewards, next_states, isdone = [], [], [], [], []\n",
    "        \n",
    "        for s, a, r, ns, done in batch:\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            next_states.append(ns)\n",
    "            isdone.append(done)\n",
    "        \n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(isdone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state, action, state_dims, action_dims, scope='critic'):\n",
    "        # state - State input to pass through the network\n",
    "        # action - Action input for which the Q value should be predicted\n",
    "         \n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.state_dims = np.prod(state_dims)       #Used to calculate the fan_in of the state layer (e.g. if state_dims is (3,2) fan_in should equal 6)\n",
    "        self.action_dims = np.prod(action_dims)\n",
    "        self.scope = scope    \n",
    "        \n",
    "        # Networks params              \n",
    "        dense1_size = 400\n",
    "        dense2_size = 300\n",
    "        final_layer_init = 0.003\n",
    "         \n",
    "        with tf.variable_scope(self.scope):           \n",
    "            self.dense1_mul = tf.layers.dense(self.state, dense1_size, kernel_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(self.state_dims))), 1/tf.sqrt(tf.to_float(self.state_dims))),\n",
    "                                bias_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(self.state_dims))), 1/tf.sqrt(tf.to_float(self.state_dims))))  \n",
    "                         \n",
    "            self.dense1 = tf.nn.relu(self.dense1_mul)\n",
    "             \n",
    "            #Merge first dense layer with action input to get second dense layer            \n",
    "            self.dense2a = tf.layers.dense(self.dense1, dense2_size, kernel_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(dense1_size+self.action_dims))), 1/tf.sqrt(tf.to_float(dense1_size+self.action_dims))),\n",
    "                                bias_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(dense1_size+self.action_dims))), 1/tf.sqrt(tf.to_float(dense1_size+self.action_dims))))        \n",
    "             \n",
    "            self.dense2b = tf.layers.dense(self.action, dense2_size, kernel_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(dense1_size+self.action_dims))), 1/tf.sqrt(tf.to_float(dense1_size+self.action_dims))),\n",
    "                                bias_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(dense1_size+self.action_dims))), 1/tf.sqrt(tf.to_float(dense1_size+self.action_dims)))) \n",
    "                           \n",
    "            self.dense2 = tf.nn.relu(self.dense2a + self.dense2b)\n",
    "                          \n",
    "            self.output = tf.layers.dense(self.dense2, 1, kernel_initializer=tf.random_uniform_initializer(-1*final_layer_init, final_layer_init),\n",
    "                                bias_initializer=tf.random_uniform_initializer(-1*final_layer_init, final_layer_init))  \n",
    "             \n",
    "                          \n",
    "            self.network_params = tf.trainable_variables(self.scope)\n",
    "          \n",
    "            self.action_grads = tf.gradients(self.output, self.action) # gradient of value output wrt action input - used to train actor network\n",
    "            \n",
    "\n",
    "    def train_step(self, target_Q):\n",
    "        # target_Q - Target Q value (immediate reward plus expected Q from next state)\n",
    "         \n",
    "        with tf.variable_scope(self.scope):\n",
    "            with tf.variable_scope('train'):\n",
    "                learning_rate = 0.001\n",
    "                l2_lambda = 0\n",
    "                 \n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "                self.loss = tf.losses.mean_squared_error(target_Q, self.output)\n",
    "                self.l2_reg_loss = tf.add_n([tf.nn.l2_loss(v) for v in self.network_params if 'kernel' in v.name]) * l2_lambda\n",
    "                self.total_loss = self.loss + self.l2_reg_loss\n",
    "                 \n",
    "                train_step = self.optimizer.minimize(self.total_loss, var_list=self.network_params)\n",
    "                  \n",
    "                return train_step\n",
    "        \n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, state, state_dims, action_dims, action_bound_low, action_bound_high, scope='actor'):\n",
    "        # state - State input to pass through the network\n",
    "        # action_bounds - Network will output in range [-1,1]. Multiply this by action_bound to get output within desired boundaries of action space\n",
    "         \n",
    "        self.state = state\n",
    "        self.state_dims = np.prod(state_dims)       #Used to calculate the fan_in of the state layer (e.g. if state_dims is (3,2) fan_in should equal 6)\n",
    "        self.action_dims = np.prod(action_dims)\n",
    "        self.action_bound_low = action_bound_low\n",
    "        self.action_bound_high = action_bound_high\n",
    "        self.scope = scope\n",
    "        \n",
    "        # Networks params \n",
    "        dense1_size = 400\n",
    "        dense2_size = 300\n",
    "        final_layer_init = 0.003\n",
    "         \n",
    "        with tf.variable_scope(self.scope):\n",
    "                    \n",
    "            self.dense1_mul = tf.layers.dense(self.state, dense1_size, kernel_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(self.state_dims))), 1/tf.sqrt(tf.to_float(self.state_dims))),\n",
    "                                bias_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(self.state_dims))), 1/tf.sqrt(tf.to_float(self.state_dims))))  \n",
    "                         \n",
    "            self.dense1 = tf.nn.relu(self.dense1_mul)\n",
    "             \n",
    "            self.dense2_mul = tf.layers.dense(self.dense1, dense2_size, kernel_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(dense1_size))), 1/tf.sqrt(tf.to_float(dense1_size))),\n",
    "                                bias_initializer=tf.random_uniform_initializer((-1/tf.sqrt(tf.to_float(dense1_size))), 1/tf.sqrt(tf.to_float(dense1_size))))        \n",
    "                         \n",
    "            self.dense2 = tf.nn.relu(self.dense2_mul)\n",
    "             \n",
    "            self.output_mul = tf.layers.dense(self.dense2, self.action_dims, kernel_initializer=tf.random_uniform_initializer(-1*final_layer_init, final_layer_init),\n",
    "                                bias_initializer=tf.random_uniform_initializer(-1*final_layer_init, final_layer_init)) \n",
    "             \n",
    "            self.output_tanh = tf.nn.tanh(self.output_mul)\n",
    "             \n",
    "            # Scale tanh output to lower and upper action bounds\n",
    "            self.output = tf.multiply(0.5, tf.multiply(self.output_tanh, (self.action_bound_high-self.action_bound_low)) + (self.action_bound_high+self.action_bound_low))\n",
    "             \n",
    "            self.network_params = tf.trainable_variables(self.scope)\n",
    "        \n",
    "        \n",
    "    def train_step(self, action_grads):\n",
    "        # action_grads - gradient of value output wrt action from critic network\n",
    "         \n",
    "        with tf.variable_scope(self.scope):\n",
    "            with tf.variable_scope('train'):\n",
    "                learning_rate = 0.0001\n",
    "                batch_size = 64\n",
    "                 \n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "                self.grads = tf.gradients(self.output, self.network_params, -action_grads)  \n",
    "                 \n",
    "                train_step = self.optimizer.apply_gradients(zip(self.grads, self.network_params))\n",
    "                 \n",
    "                return train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=0.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)    \n",
    "\n",
    "class DDPG:\n",
    "    \n",
    "    def __init__ (\n",
    "        self,\n",
    "        env,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        action_low,\n",
    "        action_high,\n",
    "        replaybuffer,\n",
    "        warm_steps = 50000,\n",
    "        tau = 0.001\n",
    "    ):\n",
    "        \n",
    "        self.env = env\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.warm_steps = warm_steps\n",
    "        self.replay = replaybuffer\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Define input placeholders    \n",
    "        self.state_ph = tf.placeholder(tf.float32, ((None,) + state_dim))\n",
    "        self.action_ph = tf.placeholder(tf.float32, ((None,) + action_dim))\n",
    "        self.target_ph = tf.placeholder(tf.float32, (None, 1))  # Target Q-value - for critic training\n",
    "        self.action_grads_ph = tf.placeholder(tf.float32, ((None,) + action_dim)) # Gradient of critic's value output wrt action input - for actor training\n",
    "        self.is_training_ph = tf.placeholder_with_default(True, shape=None)\n",
    "            \n",
    "        self.critic = Critic(self.state_ph, self.action_ph, state_dim, action_dim, scope='critic_main')\n",
    "        self.critic_target = Critic(self.state_ph, self.action_ph, state_dim, action_dim, scope='critic_target')    \n",
    "            \n",
    "        self.actor = Actor(self.state_ph, state_dim, action_dim, low, high, scope='actor_main')\n",
    "        self.actor_target = Actor(self.state_ph, state_dim, action_dim, low, high, scope='actor_target')\n",
    "        \n",
    "        # Create training step ops\n",
    "        self.critic_train_step = self.critic.train_step(self.target_ph)\n",
    "        self.actor_train_step = self.actor.train_step(self.action_grads_ph)\n",
    "\n",
    "        # Create ops to update target networks\n",
    "        self.update_critic_target = self.update_target_network(self.critic.network_params, self.critic_target.network_params, self.tau)\n",
    "        self.update_actor_target = self.update_target_network(self.actor.network_params, self.actor_target.network_params, self.tau)\n",
    "        \n",
    "    def update_target_network(self, network_params, target_network_params, tau):     \n",
    "        \n",
    "        # When tau=1.0, we perform a hard copy of parameters, otherwise a soft copy\n",
    "        # Create ops which update target network parameters with (fraction of) main network parameters\n",
    "        op_holder = []\n",
    "        for from_var,to_var in zip(network_params, target_network_params):\n",
    "            op_holder.append(to_var.assign((tf.multiply(from_var, tau) + tf.multiply(to_var, 1. - tau))))        \n",
    "\n",
    "        return op_holder\n",
    "        \n",
    "    def train(self, train_eps = 50000):\n",
    "        \n",
    "        start_ep = 0\n",
    "        sess.run(tf.global_variables_initializer())   \n",
    "        # Perform hard copy (tau=1.0) of initial params to target networks\n",
    "        sess.run(self.update_target_network(self.critic.network_params, self.critic_target.network_params, self.tau))\n",
    "        sess.run(self.update_target_network(self.actor.network_params, self.actor_target.network_params, self.tau))\n",
    "\n",
    "        # Create summary writer to write summaries to disk\n",
    "        if not os.path.exists('./logs/train'):\n",
    "            os.makedirs('./logs/train')\n",
    "        summary_writer = tf.summary.FileWriter('./logs/train', sess.graph)\n",
    "    \n",
    "        # Create summary op to save episode reward to Tensorboard log\n",
    "        ep_reward_var = tf.Variable(0.0, trainable = False)\n",
    "        tf.summary.scalar(\"Episode Reward\", ep_reward_var)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        ## Training \n",
    "        # Initially populate replay memory by taking random actions \n",
    "        sys.stdout.write('\\nPopulating replay memory with random actions...\\n')   \n",
    "        sys.stdout.flush()          \n",
    "        \n",
    "        state = self.env.reset()\n",
    "     \n",
    "        for random_step in range(1, self.warm_steps + 1):\n",
    "            action = self.env.action_space.sample()\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            self.replay.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "\n",
    "        sys.stdout.write('\\n\\nTraining...\\n')   \n",
    "        sys.stdout.flush()\n",
    "\n",
    "        for train_ep in range(start_ep + 1, train_eps + 1):      \n",
    "            # Reset environment and noise process\n",
    "            state = self.env.reset()\n",
    "            exploration_noise.reset()\n",
    "\n",
    "            train_step = 0\n",
    "            episode_reward = 0\n",
    "            duration_values = []\n",
    "            ep_done = False\n",
    "\n",
    "            sys.stdout.write('\\n')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            while not ep_done:\n",
    "                train_step += 1\n",
    "                start_time = time.time()            \n",
    "                ## Take action and store experience\n",
    "\n",
    "                action = sess.run(self.actor.output, {self.state_ph: state[None]})[0]     # Add batch dimension to single state input, and remove batch dimension from single action output\n",
    "                \n",
    "                action += exploration_noise() * noise_scaling\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay.add(state, action, reward, next_state, done)\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                ## Train networks\n",
    "                # Get minibatch\n",
    "                states_batch, actions_batch, rewards_batch, next_states_batch, done_batch = self.replay.sample(64) \n",
    "\n",
    "                # Critic training step    \n",
    "                future_action = sess.run(self.actor_target.output, {self.state_ph: next_states_batch})  \n",
    "                future_Q = sess.run(self.critic_target.output, {self.state_ph: next_states_batch, self.action_ph: future_action})[:,0]   # future_Q is of shape [batch_size, 1], need to remove second dimension for ops with terminals_batch and rewards_batch which are of shape [batch_size]\n",
    "                future_Q[done_batch] = 0\n",
    "                targets = rewards_batch + (future_Q * 0.99)\n",
    "                sess.run(self.critic_train_step, {self.state_ph:states_batch, self.action_ph:actions_batch, self.target_ph:np.expand_dims(targets, 1)})   \n",
    "\n",
    "                # Actor training step\n",
    "                actor_actions = sess.run(self.actor.output, {self.state_ph:states_batch})\n",
    "                action_grads = sess.run(self.critic.action_grads, {self.state_ph:states_batch, self.action_ph:actor_actions})\n",
    "                sess.run(self.actor_train_step, {self.state_ph:states_batch, self.action_grads_ph:action_grads[0]})\n",
    "\n",
    "                # Update target networks\n",
    "                sess.run(self.update_critic_target)\n",
    "                sess.run(self.update_actor_target)\n",
    "\n",
    "                # Display progress            \n",
    "                duration = time.time() - start_time\n",
    "                duration_values.append(duration)\n",
    "                ave_duration = sum(duration_values)/float(len(duration_values))\n",
    "\n",
    "                sys.stdout.write('\\x1b[2K\\rEpisode {:d}/{:d} \\t Steps = {:d} \\t Reward = {:.3f} \\t ({:.3f} s/step)'.format(train_ep, 50000, train_step, episode_reward, ave_duration))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "                if done or train_step == 1000:\n",
    "                    # Log total episode reward and begin next episode\n",
    "                    summary_str = sess.run(summary_op, {ep_reward_var: episode_reward})\n",
    "                    summary_writer.add_summary(summary_str, train_ep)\n",
    "                    ep_done = True\n",
    "\n",
    "        env.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkj/.conda/envs/RLsolutions/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape\n",
    "high = env.action_space.high\n",
    "low = env.action_space.low\n",
    "\n",
    "# Initialise Ornstein-Uhlenbeck Noise generator\n",
    "exploration_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "noise_scaling = 0.3 * (high - low)\n",
    "replaybuffer = ExpReplay(1e6)\n",
    "\n",
    "random_seed = 99999999\n",
    "env.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.set_random_seed(random_seed)        \n",
    "\n",
    "ddpg = DDPG(env, state_dim, action_dim, high, low, replaybuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Episode Reward is illegal; using Episode_Reward instead.\n",
      "\n",
      "Populating replay memory with random actions...\n",
      "\n",
      "\n",
      "Training...\n",
      "\n",
      "Episode 1/50000 \t Steps = 200 \t Reward = -1574.019 \t (0.021 s/step)\n",
      "Episode 2/50000 \t Steps = 200 \t Reward = -1279.673 \t (0.021 s/step)\n",
      "Episode 3/50000 \t Steps = 200 \t Reward = -1703.272 \t (0.021 s/step)\n",
      "Episode 4/50000 \t Steps = 200 \t Reward = -1417.250 \t (0.021 s/step)\n",
      "Episode 5/50000 \t Steps = 200 \t Reward = -1402.465 \t (0.021 s/step)\n",
      "Episode 6/50000 \t Steps = 200 \t Reward = -1860.237 \t (0.021 s/step)\n",
      "Episode 7/50000 \t Steps = 200 \t Reward = -1890.255 \t (0.021 s/step)\n",
      "Episode 8/50000 \t Steps = 200 \t Reward = -1678.196 \t (0.022 s/step)\n",
      "Episode 9/50000 \t Steps = 200 \t Reward = -1552.741 \t (0.021 s/step)\n",
      "Episode 10/50000 \t Steps = 200 \t Reward = -1503.556 \t (0.021 s/step)\n",
      "Episode 11/50000 \t Steps = 200 \t Reward = -1458.674 \t (0.021 s/step)\n",
      "Episode 12/50000 \t Steps = 200 \t Reward = -1540.081 \t (0.022 s/step)\n",
      "Episode 13/50000 \t Steps = 200 \t Reward = -1537.173 \t (0.022 s/step)\n",
      "Episode 14/50000 \t Steps = 200 \t Reward = -1614.208 \t (0.022 s/step)\n",
      "Episode 15/50000 \t Steps = 200 \t Reward = -1446.460 \t (0.022 s/step)\n",
      "Episode 16/50000 \t Steps = 200 \t Reward = -1494.638 \t (0.022 s/step)\n",
      "Episode 17/50000 \t Steps = 200 \t Reward = -1577.444 \t (0.024 s/step)\n",
      "Episode 18/50000 \t Steps = 200 \t Reward = -1306.058 \t (0.023 s/step)\n",
      "Episode 19/50000 \t Steps = 200 \t Reward = -1270.899 \t (0.022 s/step)\n",
      "Episode 20/50000 \t Steps = 200 \t Reward = -1470.870 \t (0.022 s/step)\n",
      "Episode 21/50000 \t Steps = 200 \t Reward = -1297.290 \t (0.022 s/step)\n",
      "Episode 22/50000 \t Steps = 200 \t Reward = -1440.582 \t (0.023 s/step)\n",
      "Episode 23/50000 \t Steps = 200 \t Reward = -1464.557 \t (0.022 s/step)\n",
      "Episode 24/50000 \t Steps = 200 \t Reward = -1593.427 \t (0.022 s/step)\n",
      "Episode 25/50000 \t Steps = 200 \t Reward = -1498.682 \t (0.022 s/step)\n",
      "Episode 26/50000 \t Steps = 200 \t Reward = -1186.267 \t (0.022 s/step)\n",
      "Episode 27/50000 \t Steps = 200 \t Reward = -1336.864 \t (0.023 s/step)\n",
      "Episode 28/50000 \t Steps = 200 \t Reward = -1499.962 \t (0.024 s/step)\n",
      "Episode 29/50000 \t Steps = 200 \t Reward = -1333.217 \t (0.023 s/step)\n",
      "Episode 30/50000 \t Steps = 200 \t Reward = -1243.425 \t (0.023 s/step)\n",
      "Episode 31/50000 \t Steps = 200 \t Reward = -1230.477 \t (0.024 s/step)\n",
      "Episode 32/50000 \t Steps = 200 \t Reward = -1122.575 \t (0.023 s/step)\n",
      "Episode 33/50000 \t Steps = 200 \t Reward = -1137.500 \t (0.023 s/step)\n",
      "Episode 34/50000 \t Steps = 200 \t Reward = -1079.594 \t (0.023 s/step)\n",
      "Episode 35/50000 \t Steps = 200 \t Reward = -1093.500 \t (0.024 s/step)\n",
      "Episode 36/50000 \t Steps = 200 \t Reward = -1089.081 \t (0.024 s/step)\n",
      "Episode 37/50000 \t Steps = 200 \t Reward = -1491.869 \t (0.024 s/step)\n",
      "Episode 38/50000 \t Steps = 200 \t Reward = -980.729 \t (0.024 s/step)\n",
      "Episode 39/50000 \t Steps = 200 \t Reward = -980.180 \t (0.024 s/step)\n",
      "Episode 40/50000 \t Steps = 200 \t Reward = -1147.696 \t (0.024 s/step)\n",
      "Episode 41/50000 \t Steps = 200 \t Reward = -1048.462 \t (0.024 s/step)\n",
      "Episode 42/50000 \t Steps = 200 \t Reward = -923.622 \t (0.024 s/step)\n",
      "Episode 43/50000 \t Steps = 200 \t Reward = -927.770 \t (0.024 s/step)\n",
      "Episode 44/50000 \t Steps = 200 \t Reward = -1244.278 \t (0.025 s/step)\n",
      "Episode 45/50000 \t Steps = 200 \t Reward = -400.237 \t (0.025 s/step)\n",
      "Episode 46/50000 \t Steps = 200 \t Reward = -530.396 \t (0.025 s/step)\n",
      "Episode 47/50000 \t Steps = 200 \t Reward = -777.950 \t (0.025 s/step)\n",
      "Episode 48/50000 \t Steps = 200 \t Reward = -1.617 \t (0.025 s/step)\n",
      "Episode 49/50000 \t Steps = 200 \t Reward = -2.130 \t (0.025 s/step)\n",
      "Episode 50/50000 \t Steps = 200 \t Reward = -895.172 \t (0.025 s/step)\n",
      "Episode 51/50000 \t Steps = 200 \t Reward = -786.068 \t (0.025 s/step)\n",
      "Episode 52/50000 \t Steps = 200 \t Reward = -956.593 \t (0.025 s/step)\n",
      "Episode 53/50000 \t Steps = 200 \t Reward = -799.684 \t (0.025 s/step)\n",
      "Episode 54/50000 \t Steps = 200 \t Reward = -804.457 \t (0.025 s/step)\n",
      "Episode 55/50000 \t Steps = 200 \t Reward = -920.629 \t (0.026 s/step)\n",
      "Episode 56/50000 \t Steps = 200 \t Reward = -771.624 \t (0.025 s/step)\n",
      "Episode 57/50000 \t Steps = 200 \t Reward = -134.106 \t (0.026 s/step)\n",
      "Episode 58/50000 \t Steps = 200 \t Reward = -519.775 \t (0.026 s/step)\n",
      "Episode 59/50000 \t Steps = 200 \t Reward = -411.847 \t (0.026 s/step)\n",
      "Episode 60/50000 \t Steps = 200 \t Reward = -140.710 \t (0.026 s/step)\n",
      "Episode 61/50000 \t Steps = 200 \t Reward = -0.975 \t (0.026 s/step)\n",
      "Episode 62/50000 \t Steps = 200 \t Reward = -261.864 \t (0.026 s/step)\n",
      "Episode 63/50000 \t Steps = 200 \t Reward = -397.729 \t (0.026 s/step)\n",
      "Episode 64/50000 \t Steps = 200 \t Reward = -411.465 \t (0.026 s/step)\n",
      "Episode 65/50000 \t Steps = 200 \t Reward = -262.029 \t (0.029 s/step)\n",
      "Episode 66/50000 \t Steps = 200 \t Reward = -226.024 \t (0.026 s/step)\n",
      "Episode 67/50000 \t Steps = 200 \t Reward = -260.385 \t (0.026 s/step)\n",
      "Episode 68/50000 \t Steps = 200 \t Reward = -263.523 \t (0.027 s/step)\n",
      "Episode 69/50000 \t Steps = 200 \t Reward = -573.559 \t (0.027 s/step)\n",
      "Episode 70/50000 \t Steps = 200 \t Reward = -385.002 \t (0.029 s/step)\n",
      "Episode 71/50000 \t Steps = 200 \t Reward = -4.323 \t (0.028 s/step)\n",
      "Episode 72/50000 \t Steps = 200 \t Reward = -553.763 \t (0.026 s/step)\n",
      "Episode 73/50000 \t Steps = 200 \t Reward = -387.710 \t (0.026 s/step)\n",
      "Episode 74/50000 \t Steps = 200 \t Reward = -647.217 \t (0.026 s/step)\n",
      "Episode 75/50000 \t Steps = 200 \t Reward = -273.385 \t (0.026 s/step)\n",
      "Episode 76/50000 \t Steps = 200 \t Reward = -268.693 \t (0.027 s/step)\n",
      "Episode 77/50000 \t Steps = 200 \t Reward = -138.349 \t (0.027 s/step)\n",
      "Episode 78/50000 \t Steps = 200 \t Reward = -10.823 \t (0.026 s/step)\n",
      "Episode 79/50000 \t Steps = 200 \t Reward = -131.628 \t (0.027 s/step)\n",
      "Episode 80/50000 \t Steps = 200 \t Reward = -425.727 \t (0.028 s/step)\n",
      "Episode 81/50000 \t Steps = 200 \t Reward = -299.373 \t (0.027 s/step)\n",
      "Episode 82/50000 \t Steps = 200 \t Reward = -620.690 \t (0.027 s/step)\n",
      "Episode 83/50000 \t Steps = 200 \t Reward = -139.696 \t (0.028 s/step)\n",
      "Episode 84/50000 \t Steps = 200 \t Reward = -278.100 \t (0.028 s/step)\n",
      "Episode 85/50000 \t Steps = 200 \t Reward = -394.822 \t (0.030 s/step)\n",
      "Episode 86/50000 \t Steps = 200 \t Reward = -126.044 \t (0.030 s/step)\n",
      "Episode 87/50000 \t Steps = 200 \t Reward = -136.237 \t (0.029 s/step)\n",
      "Episode 88/50000 \t Steps = 200 \t Reward = -134.410 \t (0.029 s/step)\n",
      "Episode 89/50000 \t Steps = 200 \t Reward = -244.777 \t (0.029 s/step)\n",
      "Episode 90/50000 \t Steps = 200 \t Reward = -132.185 \t (0.029 s/step)\n",
      "Episode 91/50000 \t Steps = 200 \t Reward = -135.112 \t (0.029 s/step)\n",
      "Episode 92/50000 \t Steps = 200 \t Reward = -136.104 \t (0.029 s/step)\n",
      "Episode 93/50000 \t Steps = 200 \t Reward = -265.979 \t (0.030 s/step)\n",
      "Episode 94/50000 \t Steps = 200 \t Reward = -6.215 \t (0.030 s/step)\n",
      "Episode 95/50000 \t Steps = 200 \t Reward = -136.009 \t (0.030 s/step)\n",
      "Episode 96/50000 \t Steps = 200 \t Reward = -134.408 \t (0.030 s/step)\n",
      "Episode 97/50000 \t Steps = 200 \t Reward = -135.235 \t (0.030 s/step)\n",
      "Episode 98/50000 \t Steps = 200 \t Reward = -266.652 \t (0.030 s/step)\n",
      "Episode 99/50000 \t Steps = 200 \t Reward = -328.540 \t (0.030 s/step)\n",
      "Episode 100/50000 \t Steps = 200 \t Reward = -242.991 \t (0.030 s/step)\n",
      "Episode 101/50000 \t Steps = 200 \t Reward = -134.270 \t (0.030 s/step)\n",
      "Episode 102/50000 \t Steps = 200 \t Reward = -131.704 \t (0.030 s/step)\n",
      "Episode 103/50000 \t Steps = 200 \t Reward = -136.131 \t (0.030 s/step)\n",
      "Episode 104/50000 \t Steps = 200 \t Reward = -266.254 \t (0.030 s/step)\n",
      "Episode 105/50000 \t Steps = 161 \t Reward = -249.781 \t (0.031 s/step)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8019aac83a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6f96e65b8547>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_eps)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mfuture_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_states_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# Predict target Q values by passing next states and actions through value target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mfuture_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_states_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture_action\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# future_Q is of shape [batch_size, 1], need to remove second dimension for ops with terminals_batch and rewards_batch which are of shape [batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# Q values of the terminal states is 0 by definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mfuture_Q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdone_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/RLsolutions/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/RLsolutions/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1137\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/RLsolutions/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \"\"\"\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/RLsolutions/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/RLsolutions/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5231\u001b[0m       \u001b[0;31m# If an exception is raised here it may be hiding a related exception in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5232\u001b[0m       \u001b[0;31m# the try-block (just above).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5233\u001b[0;31m       \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_switches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddpg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "rlsolutions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
